{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where are the sushi bars?\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The focus of my PhD research was quantitative coral reef research, and I stay in close touch with former collaborators. Erik Franklin, a researcher at UH-Manoa, is thinking about the connection between ocean ecosystems and society, and he's interested in sushi sustainability. There are several researchers and organizations working in this area, but it's surprisingly hard to get answers to simple questions. For instance, how many sushi restaurants are there in the US and where are those restaurants located?\n",
    "\n",
    "Here, I'm going to walk through how we gathered some basic data on sushi restaurants in the US.\n",
    "\n",
    "### Failed approaches\n",
    "\n",
    "One of the obvious places to get this information is from Yelp, and the easiest way is by scraping data from their website directly. If someone wanted to try this approach, they would probably know it's against Yelp's ToS and try to get creative by writing a script to make requests in a rate-limited and randomized fashion, to try and approximate human searches. They might do this because it's dead simple to search all zip codes and page through all of the results, but they'd also find out that Yelp is pretty good at identifying automated scrapers. I, of course, would never do such a thing.\n",
    "\n",
    "Another approach would be to use the Yelp API to search for sushi restaurants by zip code. Unfortunately, the API methods are not engineered in a way that facilitates exhaustive searches. The API methods are best for answering questions like, \"What are a few restaurants close to my location?\" or \"What additional information do you have for a given restaurant?\", rather than questions like \"How many restaurants are within this area?\". This is primarily because the Yelp API only returns up to 40 businesses within 40,000 meters of a given location, meaning that multiple queries are needed when the requester is interested in areas with more than 40 relevant businesses or distances greater than 40,000 meters.\n",
    "\n",
    "### Implementing a recursive, grid-based, search algorithm\n",
    "\n",
    "My solution? The Yelp API has a method for bounded searches, where one can request a list of businesses within a lat/lon box. I can divide the US into a grid where the distance from the center of each grid cell to any point within the cell is smaller than the 40,000 meter limit. I can then query the Yelp API for a list of sushi restaurants within each of those grid cells. Whenever I come to a cell where there are 40 restaurants in the response, I assume that there are more restaurants in that location than can be returned by the API; I then subdivide the cell into smaller cells and repeat the queries until I have fewer than 40 restaurants in each response and the search results are comprehensive.\n",
    "\n",
    "How will that work in practice? Grid cells in Wyoming, a landlocked state with the lowest population density in the continental US, are likely to have very few sushi restaurants and not need any recursion. The grid cells covering San Francisco are likely to have many more than 40 sushi restaurants, and it's likely that those areas will need to be subdivided multiple times to get full coverage.\n",
    "\n",
    "### Let's see the code\n",
    "\n",
    "I generally write Python scripts with the following ordering:\n",
    "- imports\n",
    "- global or hidden constants and parameters\n",
    "- high-level method to do everything\n",
    "- increasingly lower-level methods ordered by usage and/or level of complexity\n",
    "\n",
    "While this format makes it easy to maintain code, it's not ideal for demonstrating code development. Instead, I'll try to narrate code development in a somewhat logical order.\n",
    "\n",
    "First, I need a local file to save my data periodically, as well as convenience methods for saving and loading that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Local file for saving data results\n",
    "PATH_DATA_MASTER = '/Users/nsfabina/yelp_sushi/data_master.json'\n",
    "\n",
    "\n",
    "def save_master_data(data_master):\n",
    "    with open(PATH_DATA_MASTER, 'w') as file_:\n",
    "        json.dump(data_master, file_)\n",
    "        \n",
    "        \n",
    "def load_master_data():\n",
    "    with open(PATH_DATA_MASTER) as file_:\n",
    "        data_master = json.load(file_)\n",
    "    return data_master\n",
    "\n",
    "\n",
    "# Create empty file\n",
    "save_master_data(dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, I'm going to use a third-party library to authenticate with the Yelp servers, and I need to set some search parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yelp\n",
    "from yelp.client import Client\n",
    "from yelp.oauth1_authenticator import Oauth1Authenticator\n",
    "\n",
    "\n",
    "# Yelp auth - obscured\n",
    "CONSUMER_KEY = '*'\n",
    "CONSUMER_SECRET = '*'\n",
    "TOKEN = '*'\n",
    "TOKEN_SECRET = '*'\n",
    "YELP_CLIENT = Client(Oauth1Authenticator(\n",
    "    consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET, token=TOKEN,\n",
    "    token_secret=TOKEN_SECRET))\n",
    "\n",
    "# Yelp API params\n",
    "API_SORT = '1'\n",
    "API_CATEGORY_FILTER = 'sushi'\n",
    "API_PARAMS = {'sort': API_SORT, 'category_filter': API_CATEGORY_FILTER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, I need to set location parameters for my grid-based search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Brute force US latlon ranges - because it's not worth the extra investment to check whether\n",
    "# a grid point is on US territory or in the middle of the ocean\n",
    "MIN_LAT = 17.75\n",
    "MAX_LAT = 49.25\n",
    "MIN_LON = -124.75\n",
    "MAX_LON = -62.50\n",
    "\n",
    "# Distance conversions\n",
    "# Note that I just assume constant degrees from the equator to the poles, even though longitude \n",
    "# degrees cover more area closer to the equator. Instead, I use values for degree conversion from\n",
    "# the equator and realize that my grid will have more cells than necessary from east to west.\n",
    "# I'm also setting a buffer because I don't know whether Yelp's distance limit is as the crow\n",
    "# flies, as the car drives, or whether some crazy engineer came up with a wonkier definition, so\n",
    "# I'm just going to play it safe.\n",
    "API_DISTANCE_LIMIT_METERS = 40000.0\n",
    "DEGREES_TO_METERS = 111111.11\n",
    "GLOBAL_GRID_DISTANCE = API_DISTANCE_LIMIT_METERS / DEGREES_TO_METERS\n",
    "BUFFER_DISTANCE = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use a GridPoint class to hold all of the location information for the global and location gridpoints, and that class will calculate search bounds knowing just the central lat/lon and resolution level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridPoint(object):\n",
    "    def __init__(self, lon_center, lat_center, resolution):\n",
    "        self.lon_center = lon_center\n",
    "        self.lat_center = lat_center\n",
    "        self.resolution = resolution\n",
    "        self._set_bounds()\n",
    "    \n",
    "    def _set_bounds(self):\n",
    "        distance = GLOBAL_GRID_DISTANCE / self.resolution / 2.0  # Half the distance across the grid cell\n",
    "        self.bounds = {\n",
    "            'min_lat': self.lat_center-distance, 'min_lon': self.lon_center-distance,\n",
    "            'max_lat': self.lat_center+distance, 'max_lon': self.lon_center+distance,\n",
    "        }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global grid will be an iterator of GridPoints across the US, with latitude and longitude step sizes defined by the Yelp distance limit. I'm going to use numpy to generate those degree vectors, even though it's totally possible to do that with vanilla Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_global_grid():\n",
    "    resolution_start = 1.0\n",
    "    return iter(GridPoint(lon, lat, resolution_start)\n",
    "                for lon in np.arange(MIN_LON, MAX_LON, BUFFER_DISTANCE * GLOBAL_GRID_DISTANCE)\n",
    "                for lat in np.arange(MIN_LAT, MAX_LAT, BUFFER_DISTANCE * GLOBAL_GRID_DISTANCE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local grids will be iterators of GridPoints as well. Local grids will be created whenever a grid cell has more restaurants than can be returned by the Yelp API. In this case, I subdivide a grid cell into four, equally-sized subcells by dividing it in half north-south and east-west. I also need to define the new grid cells as having greater resolution so that the bounds are adjusted as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_local_grid(point):\n",
    "    distance = GLOBAL_GRID_DISTANCE / point.resolution / 2.0  # Half the distance across the grid cell\n",
    "    local_distance = distance / 2.0  # Midpoint for splitting the grid cell into four points\n",
    "    return iter(GridPoint(point.lon_center + (local_distance * x),\n",
    "                          point.lat_center + (local_distance * y),\n",
    "                          point.resolution * 2.0)\n",
    "                for x in (-1, 1) for y in (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous components are more-or-less independent from one another, while everything from this point forward is going to be interconnected. For clarity, I'll show the high-level function that I call for each grid cell in the global grid, and then I'll walk through each of the methods it calls. Basically, I build the global grid and iterate through each cell:\n",
    "- query the Yelp API for the current cell,\n",
    "- parse the businesses from the response,\n",
    "- check if the response returned all businesses,\n",
    "    - if the search was exhaustive, move to the next cell OR\n",
    "    - subdivide the cell and conduct additional searches\n",
    "\n",
    "You'll notice this is a recursive algorithm that terminates when the Yelp API has returned all sushi restaurants in an area of interest, and you'll also notice that I update my data object after each action is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_businesses_nearby_point(point, data_master):\n",
    "    response = query_point_for_businesses(point, data_master)\n",
    "    data_master = update_master_data_points(point, data_master)\n",
    "    if response is None:\n",
    "        return data_master\n",
    "    businesses = parse_businesses(response)\n",
    "    data_master = update_master_data_businesses(businesses, data_master)\n",
    "    if check_is_exhausted(response) is False:\n",
    "        grid_local = create_local_grid(point)\n",
    "        for recursed_point in grid_local:\n",
    "            data_master = get_businesses_nearby_point(recursed_point, data_master)\n",
    "    return data_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the lower-level methods in the order they're called.\n",
    "\n",
    "When I query each point, I want to catch any potential errors. I want to stop if I've used all my API calls or if I see an unexpected error, while I just want to log the other, expected Yelp errors. The latter errors are things like, \"Yelp screwed up\" or \"Nick requested data from the middle of the ocean\". I'll save data before the program terminates so that I can pick up where I left off, and I'll print points with unexpected errors for easier debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERRORS_CRITICAL = (yelp.errors.ExceededReqs)\n",
    "ERRORS_LOG =(yelp.errors.InternalError,\n",
    "             yelp.errors.InvalidParameter,\n",
    "             yelp.errors.UnavailableForLocation)\n",
    "\n",
    "\n",
    "def query_point_for_businesses(point, data_master):\n",
    "    try:\n",
    "        response = YELP_CLIENT.search_by_bounding_box(\n",
    "            point.bounds['min_lat'], point.bounds['min_lon'], point.bounds['max_lat'],\n",
    "            point.bounds['max_lon'], **API_PARAMS)\n",
    "    except ERRORS_CRITICAL as error:\n",
    "        print(error)\n",
    "        save_master_data(data_master)\n",
    "        raise error\n",
    "    except ERRORS_LOG as error:\n",
    "        data_master = update_master_data_errors(error, point, data_master)\n",
    "        response = None\n",
    "    except Exception as error:\n",
    "        print(point.lon_center, point.lat_center, point.resolution)\n",
    "        print(error)\n",
    "        save_master_data(data_master)\n",
    "        raise error\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When responses are returned successfully, I want to parse all of the business information and convert the Yelp classes to pure JSON for saving. While I could pickle these Yelp classes, there are too many gotchas with pickling that I'd like to avoid, such as changes to the data definitions for Yelp or for my own code. Notice that I'm not going to clean or modify any data, making it easier for me to be flexible with how data is used downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_businesses(response):\n",
    "    return {business.id: parse_business(business) for business in response.businesses}\n",
    "\n",
    "\n",
    "def parse_business(business):\n",
    "    parsed = dict()\n",
    "    if business.location is not None:\n",
    "        parsed.update({\n",
    "            'address': business.location.address,\n",
    "            'city': business.location.city,\n",
    "            'country_code': business.location.country_code,\n",
    "            'postal_code': business.location.postal_code,\n",
    "            'state_code': business.location.state_code,\n",
    "            })\n",
    "    if business.location.coordinate is not None:\n",
    "        parsed.update({\n",
    "            'latitude': business.location.coordinate.latitude,\n",
    "            'longitude': business.location.coordinate.longitude,\n",
    "            })\n",
    "    parsed.update({\n",
    "        'categories': [(cat.alias, cat.name) for cat in business.categories],\n",
    "        'phone_display': business.display_phone,\n",
    "        'id': business.id,\n",
    "        'is_claimed': business.is_claimed,\n",
    "        'is_closed': business.is_closed,\n",
    "        'name': business.name,\n",
    "        'phone': business.phone,\n",
    "        'url': business.url,\n",
    "       })\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be confident that a search is exhausted -- that all restaurants in a grid cell are known -- if there are fewer than 40 businesses in the response. If there are 40 businesses in the response, we can't determine whether the search was exhausted and there were exactly 40 businesses in the grid cell or if the search is incomplete and there were 41 or more businesses. In either case, we need to recurse to find any unknown sushi restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_BUSINESS_LIMIT = 40\n",
    "\n",
    "def check_is_exhausted(response):\n",
    "    if response.total < API_BUSINESS_LIMIT:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have some convenience methods for updating the data object. Basically, I have a dictionary with keys for businesses, all points in the order they were searched, the last global grid point completed, and the errors that were observed and which points they were associated with. Note that I assume Yelp has unique internal IDs for every business -- a safe assumption -- and that businesses are added to a dictionary with this internal ID as a key. Thus, the final list of businesses with only have unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_master_data_businesses(businesses, data_master):\n",
    "    data_master.setdefault('businesses', {})\n",
    "    data_master['businesses'].update(businesses)\n",
    "    return data_master\n",
    "\n",
    "\n",
    "def update_master_data_points(point, data_master):\n",
    "    data_master.setdefault('points', [])\n",
    "    data_master['points'].append(jsonify_point(point))\n",
    "    return data_master\n",
    "    \n",
    "\n",
    "def update_master_data_last_global_point(point_global, data_master):\n",
    "    data_master['last_complete_global_point'] = jsonify_point(point_global)\n",
    "    return data_master\n",
    "\n",
    "\n",
    "def update_master_data_errors(error, point, data_master):\n",
    "    error_dict = json.loads(str(error).replace('\\'', '\"').replace('u\\\"', '\"'))\n",
    "    error_id = error_dict['id']\n",
    "    data_error = data_master.setdefault('errors', {}).setdefault(error_id, [])\n",
    "    data_error.append(jsonify_point(point))\n",
    "    return data_master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost there! I just have a couple of convenience methods for interacting with data. First, two methods for converting points to JSON and back to the class representation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jsonify_point(point):\n",
    "    return {'lat_center': point.lat_center, 'lon_center': point.lon_center,\n",
    "            'resolution': point.resolution}\n",
    "\n",
    "\n",
    "def classify_point(point_json):\n",
    "    return GridPoint(point_json['lon_center'], point_json['lat_center'],\n",
    "                     point_json['resolution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and finally, a method for finding the starting point in the global grid when (not if) my program inevitably needs to be restarted. Restarts will happen when the API call limit is reached or when I run into unexpected errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_to_starting_global_point(grid_global, data_master):\n",
    "    last_point = data_master.get('last_complete_global_point')\n",
    "    if last_point is None:\n",
    "        return\n",
    "    last_point = classify_point(last_point)\n",
    "    for idx, point in enumerate(grid_global):\n",
    "        if (point.lat_center == last_point.lat_center and \n",
    "                point.lon_center == last_point.lon_center and \n",
    "                point.resolution == last_point.resolution):\n",
    "            print('Points skipped: {}'.format(idx))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby, eh? The only concern I have is not knowing how long it's going to take to complete my searches. There are at least 65,000 points to search in the national grid, plus as many points in the recursions for exhauastive searches. I have a limit of 25,000 calls per day, so it just depends on how deep I need to go for complete coverage.\n",
    "\n",
    "Below is my snippet for running this code. The method calls are well-named, but in plain English:\n",
    "- load the existing data,\n",
    "- create the global grid,\n",
    "- find the starting point in the global grid,\n",
    "- query each point in order\n",
    "- save the data after each 100 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_master = load_master_data()\n",
    "grid_global = create_global_grid()\n",
    "iterate_to_starting_global_point(grid_global, data_master)\n",
    "for idx, point_global in enumerate(grid_global):\n",
    "    print '\\r', idx,\n",
    "    get_businesses_nearby_point(point_global, data_master)\n",
    "    data_master = update_master_data_last_global_point(point_global, data_master)\n",
    "    if idx % 100 == 0:\n",
    "        save_master_data(data_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save after each 100 queries because writes add significant overhead when data objects get larger. The absolute time to save is short, but the time relative to rerunning queries is long.\n",
    "\n",
    "### What next?\n",
    "\n",
    "Erik and I are going to create a visualization of sushi restaurants in the US after this is complete, as well as use the raw data for specific research purposes. Keep an eye out for the former in an updated version of this notebook, and the latter in the pages of _Science_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
