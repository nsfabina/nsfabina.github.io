{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping code maintainable and extensible for small teams\n",
    "\n",
    "## Introduction\n",
    "\n",
    "For ten years, I've been working on small teams where people have different areas of expertise. The world expert in domain X may not be comfortable doing anything more than building models in their favorite language, while the bright, polyglot programmer may have no interest in analytics. When you need to build a new model, do you need your data scientists to be comfortable with your programming frameworks, distributed systems, or microservices? When you need to modify your infrastructure, do you need your engineers to understand how data is cleaned and formatted, or how models are trained and crossvalidated? How do you build a system that is maintainable and extensible?\n",
    "\n",
    "As first engineer at my last startup, I was responsible for creating both the engineering and data science components of our data analytics pipeline. I chose to keep these components separated so that team members with strengths in either area would feel comfortable maintaining and extending the codebase, and I'm going to demonstrate how with some sample code.\n",
    "\n",
    "## Project structure\n",
    "\n",
    "First, this was a Django project, so I created an ```analytics``` module in the ```main``` folder beside ```commands```, ```views```, and other standard Django modules. The project looks somewhat like this, with more descriptive names for the generic \"model a\", \"model b\", and \"model c\" labels:\n",
    "\n",
    "```\n",
    "├──  analytics\n",
    "│   ├──  model_base.py\n",
    "│   ├──  model_a\n",
    "│   │   ├──  base.py\n",
    "│   │   ├──  dataset.py\n",
    "│   │   ├──  model.py\n",
    "│   ├──  model_b\n",
    "│   │   ├──  base.py\n",
    "│   │   ├──  dataset.py\n",
    "│   │   ├──  model.py\n",
    "│   ├──  model_c\n",
    "│   │   ├──  base.py\n",
    "│   │   ├──  dataset.py\n",
    "│   │   ├──  model.py\n",
    "```\n",
    "\n",
    "(Thanks ```brew install tree```!)\n",
    "\n",
    "The general idea is that the engineering code is restricted to ```model_base.py``` and the ```base.py``` scripts, while the data science code is found only in the ```dataset.py``` and ```model.py``` scripts. Do you need to do something with AWS EFS? Do you need to work with scikit-learn? You already know where you should be looking.\n",
    "\n",
    "## Engineering components\n",
    "\n",
    "In ```model_base.py```, I've created a few resources that are used by each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sample(object):\n",
    "    \"\"\"\n",
    "    I'm a class for uniform dataset generation. Engineering scripts know how to assign\n",
    "    cleaned and formatted data to me as attributes, while data science scripts know how\n",
    "    to access attributes to generate Datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_id, sample_data, sample_label):\n",
    "        self.id = sample_id\n",
    "        self.data = sample_data\n",
    "        self.label = sample_label\n",
    "    \n",
    "    \n",
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    I'm a class for uniform model fitting and predicting, or training and testing. Data\n",
    "    science scripts know how to assign features and labels to me as attributes, while models\n",
    "    know how to fit and predict using those attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_ids, features, labels):\n",
    "        self.sample_ids = sample_ids\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "\n",
    "class Prediction(object):\n",
    "    \"\"\"\n",
    "    I'm a class for uniform model output. Data science scripts know how to assign predictions\n",
    "    and probabilities, while engineering scripts know how to access those values for business\n",
    "    logic use-cases.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_id, model_label, model_probability):\n",
    "        self.id = sample_id\n",
    "        self.label = model_label\n",
    "        self.probability = model_probability\n",
    "        \n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    I'm a class to make it simple to fit and predict.\n",
    "    \"\"\"\n",
    "    def fit(self, dataset_fit):\n",
    "        self.dataset_fit = dataset_fit\n",
    "        self.model = self._get_model()\n",
    "        self._fit(dataset_fit, self.model)\n",
    "        \n",
    "    def _fit(self, dataset_fit, model):\n",
    "        \"\"\"\n",
    "        I need to be implemented for each derived model class, and I need to accept Datasets\n",
    "        generated from all Samples.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, dataset_predict):\n",
    "        return self._predict(dataset_predict, self.model)\n",
    "    \n",
    "    def _predict(self, dataset_predict, model):\n",
    "        \"\"\"\n",
    "        I need to be implemented for each derived model class, and I need to return\n",
    "        Predictions for each sample.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "def save_model(model):\n",
    "    \"\"\"\n",
    "    I save models in a standardized way.\n",
    "    \"\"\"\n",
    "    # Not shown:  code to save models\n",
    "    \n",
    "\n",
    "def load_model(model_identifier):\n",
    "    \"\"\"\n",
    "    I load models in a standardized way.\n",
    "    \"\"\"\n",
    "    # Not shown:  code to load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice side-effect of this partitioning? Every internal model will now have the same pipeline and API:\n",
    "\n",
    "1. Generate data\n",
    "2. Create Samples from raw data\n",
    "3. Create a Dataset from Samples\n",
    "4. Create a Model from a Dataset\n",
    "5. Create Predictions from a Model and new Samples/Datasets\n",
    "\n",
    "Sure, there are lower-level details that I'm glossing over, like the fact that Dataset.features can be things like ```pandas.DataFrames``` or ```scipy.SparseDataFrames```, but we're going for a high-level overview.\n",
    "\n",
    "Next, the ```base.py``` scripts create models, predict with existing models, and crossvalidate models. The pattern generally follows something like the following, and could definitely be refactored and generalized in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from main.analytics import model_base\n",
    "import main.analytics.model_a.dataset as dataset_a\n",
    "import main.analytics.model_a.model as model_a\n",
    "\n",
    "\n",
    "def create_model(samples):\n",
    "    dataset_fit = dataset_a.get_dataset_fit(samples)\n",
    "    model = model_a.DerivedModel()\n",
    "    model.fit(dataset_fit)\n",
    "    model_base.save_model(model)\n",
    "    \n",
    "\n",
    "def predict_with_model(samples):\n",
    "    dataset_predict = dataset_a.get_dataset_predict(samples)\n",
    "    model = model_base.load_model(model_id)  # We set model ID elsewhere, not important\n",
    "    return model.predict(dataset_predict)\n",
    "\n",
    "\n",
    "def crossvalidate_model(samples):\n",
    "    \"\"\"\n",
    "    This function has code for training and testing, generates performance reports\n",
    "    \"\"\"\n",
    "    # Not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data science components\n",
    "\n",
    "Does the code involve math, statistics, or machine learning? Does it use numpy, pandas, scipy, or scikit-learn? Does it generate or format features, labels, or models? Then it'll be found in either a ```dataset.py``` or ```model.py``` script. The former generally looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import scipy\n",
    "\n",
    "from main.analytics.model_base import Dataset\n",
    "\n",
    "\n",
    "def get_dataset_fit(samples):\n",
    "    # Simplified example:\n",
    "    sample_ids = [sample.id for sample in samples]\n",
    "    features = parse_features_from_samples(samples)\n",
    "    labels = parse_labels_from_samples(samples)\n",
    "    return Dataset(sample_ids=sample_ids, features=features, labels=labels)\n",
    "\n",
    "\n",
    "def get_dataset_predict(samples):\n",
    "    # Simplified example\n",
    "    features = parse_features_from_samples(samples)\n",
    "    return Dataset(features=features)\n",
    "\n",
    "\n",
    "def parse_features_from_samples(samples):\n",
    "    stuff = numpy.foo(samples)\n",
    "    more_stuff = pandas.bar(stuff)\n",
    "    return scipy.baz(more_stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the latter, ```model.py```, generally follows this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.model_module\n",
    "\n",
    "from main.analytics.model_base import Model, Prediction\n",
    "\n",
    "\n",
    "class DerivedModel(Model):\n",
    "    \n",
    "    def _get_model(self):\n",
    "        return sklearn.model_module.model_class()\n",
    "    \n",
    "    def _fit(self, dataset_fit, model):\n",
    "        model.fit(dataset_fit.features, dataset_fit.labels)\n",
    "    \n",
    "    def _predict(self, dataset_predict, model):\n",
    "        predictions = model.predict(dataset_predict.features)\n",
    "        probabilities = model.predict_proba(dataset_predict.labels)\n",
    "        return [Prediction(id_, label, prob) for id_, label, prob\n",
    "                in zip(dataset_predict.sample_ids, predictions, probabilities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the bigger picture\n",
    "\n",
    "You need to modify how or where models are being stored, or how often they're being updated? Piece of cake! Have an engineer modify the base ```save_model()``` and ```load_model``` functions. They won't need to know how raw data is being used or where model predictions are coming from; whether the results are coming from linear models, neural networks, or random number generators.\n",
    "\n",
    "You need to create a brand-new model for a new business goal, or improve the peformance of an existing model? No problem! Have a data scientist create a new ```Dataset``` and ```DerivedModel``` object, or modify the methods on existing objects. They won't need to care whether their model state is stored locally, on S3 or EFS, or the CEO's personal computer. \n",
    "\n",
    "This isn't the perfect solution -- there are definitely improvements to be made -- but it's a good starting point for a small team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
