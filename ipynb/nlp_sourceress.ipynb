{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification using natural language processing\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Our goal at Sourceress is to improve the recruiting process so that people can find meaningful  positions. Everyone knows about the opportunities available at large tech corporations and popular unicorns, but there are a fair number of young and growing companies with compelling missions. We want to be sure that mission-driven individuals can find and join teams where they share a common goal.\n",
    "\n",
    "I work on backend, frontend, and devops as a software engineer at Sourceress, but my most recent project was to develop a data science tool to predict candidate fit at a startup using natural language processing tools. I'd like to share how I approached this challenge below.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "I have a dataset of candidates for a role at a Bay Area startup, with a profile for each canididate. Along with those profiles, I have a record of which candidates were judged to be a fit for the role, denoted by being \"APPROVED\" or \"REJECTED\". The question is:  can I write a classifier that can take unstructured profile data and estimate the probability that a candidate is approved or rejected?\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Jupyter notebook works great with Django. Just follow the instructions for installing Jupyter at their website, install django_extensions using pip, and then instantiate a notebook at the root of your project with:\n",
    "\n",
    "```python manage.py shell_plus --notebook```\n",
    "\n",
    "Once in the notebook, I'm going to initialize my project with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import django\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing and cleaning data\n",
    "\n",
    "We need to access and filter internal data prior to analyses, but I'm going to gloss over this section because interactions with our internal API aren't that interesting. The general steps are:\n",
    "- loading Django models, \n",
    "- querying database via object filters,\n",
    "- removeing questionable data, and\n",
    "- aggregating text content with convenience functions.\n",
    "\n",
    "The critical result is that we will have an object called \"documents\" with data IDs and text content for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating features using NLTK\n",
    "\n",
    "With these documents, we can generate features for analyses. Spacy and NLTK are two Python libraries which are commonly recommended for NLP. I explored Spacy first because it was recommended for being fast and lightweight, but I found that it was lacking necessary functionality and was eating ~2GB of memory upon being loaded. I switched to NLTK because it had the out-of-the-box tools I needed and allowed me to be discerning about which modules I loaded.\n",
    "\n",
    "We'll download and import NLTK resources for tokenization, stopwords, and lemmatization. Then we'll create a set of stopwords to remove low-information tokens and a lemmatizer to \"normalize\" tokens, with the ultimate goal of reducing our language size for more effective analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenization\n",
    "assert nltk.download('punkt') is True, 'Download of NLTK resource \"punkt\" has failed.'\n",
    "# Stopwords\n",
    "assert nltk.download('stopwords') is True, 'Download of NLTK resource \"stopwords\" has failed.'\n",
    "# Lemmatization\n",
    "assert nltk.download('wordnet') is True, 'Download of NLTK resource \"wordnet\" has failed.'\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "STOPSET = set(stopwords.words('english'))\n",
    "LEMMATIZER = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to clean our documents so that the NLTK tokenization works as expected, and to further reduce language size. Comments in the code below explain the rational behind each cleaning step, and it's worth reminding ourselves that the text content has many technical terms that need special treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# I don't want to rely on the NLTK tokenizer to correctly parse phrases like\n",
    "# \".NET\" or \"ASP.NET\", and I also want to clean web addresses, so I'm just going \n",
    "# to change those phrases such that the \".\" is spelled out. Same thing with\n",
    "# \"node.js\", \"handlebars.js\", and other variants.\n",
    "\n",
    "def _clean_dot_net(text):\n",
    "    # Note: re's look-behind's need to be fixed width\n",
    "    options = [r'((?<=\\b(ado|asp))\\.)', r'((?<=^)\\.)', r'((?<=\\s)\\.)']\n",
    "    regex = r'(' + r'|'.join(options) + r')(?=net\\b)(?!/)'\n",
    "    replace = 'dot'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "\n",
    "def _clean_dot_js(text):\n",
    "    # Necessary because of link cleaning\n",
    "    regex = r'\\.(?=js\\b)'\n",
    "    replace = 'dot'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "# Go figure, there are a lot of websites associated with the candidates in\n",
    "# our dataset. It doesn't make sense to keep these as unique tokens right now,\n",
    "# so I'm going to turn them into generic website names and see if a model can\n",
    "# do anything with them before digging deeper.\n",
    "\n",
    "def _clean_web_addresses(text):\n",
    "    regex = r'\\b(http|www|\\.\\w{3})\\S*\\b'\n",
    "    replace = 'defaultwebaddr'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "# The NLTK tokenizer doesn't break on slashes, so someone who might be a\n",
    "# \"data scientist/software engineer\" will have tokens for \"data\",\n",
    "# \"scientist/software\", and \"engineer\". Thus, I'm going to convert slashes to\n",
    "# spaces to coerce tokenization in these instances.\n",
    "\n",
    "def _clean_slash(text):\n",
    "    regex = r'/'\n",
    "    replace = ' '\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "# Two cleaning functions for special characters that are important, but are\n",
    "# not handled correctly by NLTK tokenization or str.is_alpha() filters.\n",
    "\n",
    "def _clean_plus(text):\n",
    "    # Examples:  google+, c++\n",
    "    regex = r'\\+'\n",
    "    replace = 'plus'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "\n",
    "def _clean_c_sharp(text):\n",
    "    regex = r'\\bc#\\B'\n",
    "    replace = 'defaultcsharp'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "# Finally, there are an infinite number of numeric tokens that may appear.\n",
    "# I'm going to clean up all of the common cases so that our language is\n",
    "# reduced even further, starting with specific types of numeric information\n",
    "# and ending with generic numbers.\n",
    "\n",
    "def _clean_years(text):\n",
    "    regex = r'\\b(19|20)\\d{2}\\b'\n",
    "    replace = 'defaultyear'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "\n",
    "def _clean_currency(text):\n",
    "    regex = '\\$(\\+|-)?(?:0|[1-9]\\d{0,2}(?:,?\\d{3})*)(?:\\.\\d+)?'\n",
    "    replace = 'defaultcurrency'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "\n",
    "def _clean_percent(text):\n",
    "    regex = '(\\+|-)?(?:0|[1-9]\\d{0,2}(?:,?\\d{3})*)(?:\\.\\d+)?%'\n",
    "    replace = 'defaultpercent'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "\n",
    "def _clean_rank(text):\n",
    "    regex = '#\\d+'\n",
    "    replace = 'defaultrank'\n",
    "    return re.sub(regex, replace, text)\n",
    "\n",
    "\n",
    "def _clean_numbers(text):\n",
    "    regex = r'\\b\\d+\\b'\n",
    "    replace = 'defaultnumber'\n",
    "    return re.sub(regex, replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've probably noticed that there are potential collisions between these cleaning functions - that our results are dependent on the order in which the functions are applied to text. For instance, if we replaced slashes with spaces before replacing web addresses, we would be generating a soup of nonsense tokens from longer links. Thus, we will be careful to apply cleaning functions in a particular order.\n",
    "\n",
    "Below, we'll create a function that cleans documents, tokenizes the resulting text, removes stopwords and non-alphabetic tokens, and lemmatizes the remaining tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_tokens_from_document(document):\n",
    "    cleaning_functions = [\n",
    "        _clean_dot_net, _clean_dot_js, _clean_web_addresses, _clean_slash, _clean_plus,\n",
    "        _clean_c_sharp, _clean_years, _clean_currency, _clean_percent, _clean_rank,\n",
    "        _clean_numbers\n",
    "        ]\n",
    "    for fxn in cleaning_functions:\n",
    "        document = fxn(document)\n",
    "    # Return lemmatized tokens if alphabetic and not in the stopset\n",
    "    return [LEMMATIZER.lemmatize(token) for token in nltk.word_tokenize(document)\n",
    "            if token not in STOPSET and token.isalpha()]\n",
    "\n",
    "tokens = {person_id: parse_tokens_from_document(document)\n",
    "          for person_id, document in documents.items()}\n",
    "\n",
    "print('Sample tokens from a single document')\n",
    "for token in list(tokens.values())[0][:10]:\n",
    "    print('  ' + token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a list of tokens for each document. We can use this to generate a list of common unigrams to use as features, and we'll require that included unigrams occur in at least 100 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten unigrams returned:\n",
      " ['top', 'flex', 'opengl', 'decision', 'implementing', 'look', 'gateway', 'idea', 'capability', 'including']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Minimum number of document occurrences needed for inclusion\n",
    "MIN_TOKEN_COUNT = 100\n",
    "\n",
    "\n",
    "def get_unigrams_from_tokens(tokens):\n",
    "    # Count the number of times a token occurs at least once in a document\n",
    "    counter = collections.Counter()\n",
    "    for tkns in tokens.values():\n",
    "        counter.update(set(tkns))\n",
    "    # Return a list of unigrams that meet the inclusion criteria\n",
    "    return [token for token, count in counter.items() if count > MIN_TOKEN_COUNT]\n",
    "\n",
    "\n",
    "unigrams = get_unigrams_from_tokens(tokens)\n",
    "\n",
    "print('Sample unigrams')\n",
    "for unigram in unigrams[:10]:\n",
    "    print('  ' + unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We also want a list of bigrams. More specifically, we want a list of bigrams that occur more often than expected by chance, also known as collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten bigrams returned:\n",
      " [['day', 'per'], ['page', 'single'], ['cs', 'mysql'], ['computing', 'distributed'], ['facebook', 'social'], ['many', 'project'], ['defaultwebaddr', 'defaultyear'], ['build', 'design'], ['javascript', 'using'], ['design', 'pattern']]\n"
     ]
    }
   ],
   "source": [
    "# Token window for collocation identification\n",
    "COLLOCATION_WINDOW = 5  \n",
    "# Minimum likelihood ratio for collocation inclusion\n",
    "MIN_LIKELIHOOD_RATIO = 10  \n",
    "\n",
    "\n",
    "def get_collocated_bigrams_from_tokens(tokens): \n",
    "    # Get all bigrams that occur within a rolling window of text\n",
    "    finder_module = nltk.collocations.BigramCollocationFinder\n",
    "    finder = finder_module.from_words([tkn for tkns in tokens.values() for tkn in tkns],\n",
    "                                      COLLOCATION_WINDOW)\n",
    "    # Only include bigrams that occur a certain number of ties\n",
    "    finder.apply_freq_filter(MIN_TOKEN_COUNT)\n",
    "    # Return a list of unique bigrams that meet the likelihood criteria\n",
    "    measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigrams = set([' '.join(sorted([first_word, second_word]))\n",
    "                   for (first_word, second_word), score\n",
    "                   in finder.score_ngrams(measures.likelihood_ratio)\n",
    "                   if first_word != second_word and score > MIN_LIKELIHOOD_RATIO])\n",
    "    return [re.split(' ', bigram) for bigram in bigrams]\n",
    "\n",
    "\n",
    "bigrams = get_collocated_bigrams_from_tokens(tokens)\n",
    "\n",
    "print('Sample bigrams')\n",
    "for bigram in bigrams[:10]:\n",
    "    print('  ' + bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of common unigrams and bigrams, we can prune each document's tokens to generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(tokens, unigrams, bigrams):\n",
    "    # Create a dictionary to easily reference bigrams using either word in each pair\n",
    "    bigram_hash = {}\n",
    "    for tkn_x, tkn_y in bigrams:\n",
    "        bigram_hash.setdefault(tkn_x, set()).update([tkn_y])\n",
    "        bigram_hash.setdefault(tkn_y, set()).update([tkn_x])\n",
    "    # Step through each document's tokens, locating all unigram and bigram occurrences\n",
    "    features = {}\n",
    "    for person_id, tkns in tokens.items():\n",
    "        # Get unigrams that occur using a set for quick lookups\n",
    "        tkn_feats = set(unigram for unigram in unigrams if unigram in set(tkns))\n",
    "        # Add bigrams that occur by stepping through tokens\n",
    "        window = COLLOCATION_WINDOW - 1\n",
    "        for idx_x, tkn_x in enumerate(tkns):\n",
    "            # Continue to the next token if the focal token does not occur in any bigrams\n",
    "            if tkn_x not in bigram_hash:  \n",
    "                continue\n",
    "            # Check the collocation window for tokens found in bigrams with the focal token\n",
    "            idx_start = max(0, idx_x - window)\n",
    "            idx_end = min(len(tkns), idx_x + window)\n",
    "            for tkn_y in tkns[idx_start:idx_end]:\n",
    "                if tkn_y in bigram_hash[tkn_x]:\n",
    "                    # Store bigram with words alphabetically ordered\n",
    "                    tkn_feats.update([' '.join(sorted([tkn_x, tkn_y]))])  \n",
    "        features[person_id] = tkn_feats\n",
    "    return features\n",
    "\n",
    "\n",
    "features = get_features(tokens, unigrams, bigrams)\n",
    "\n",
    "print('Sample features')\n",
    "for feature in list(features.values())[:10]:\n",
    "    print('  ' + feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a Scipy sparse dataframe from these features because they work well with Scikit-Learn models. We choose sparse dataframes because the number of features will increase as our corpus size grows, and we don't want to run into memory issues too quickly. Unfortunately, sparse dataframes do not store row or column labels, so we track these explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "# Column names are feature strings\n",
    "feature_columns = {unigram: idx_col for idx_col, unigram in enumerate(unigrams)}\n",
    "feature_columns.update({' '.join(sorted(bigram)): idx_col + len(unigrams)\n",
    "                        for idx_col, bigram in enumerate(bigrams)})\n",
    "\n",
    "# We need to generate a new data format for the sparse dataframe API\n",
    "data = []  # Track data values\n",
    "data_row = []  # Track row indices for data values\n",
    "data_col = []  # Track column indices for data values\n",
    "\n",
    "# Row names are data IDs\n",
    "feature_rows = {}\n",
    "\n",
    "# Iterate through rows and generate new data objects\n",
    "for idx_row, (person_id, ftrs) in enumerate(features.items()):\n",
    "    feature_rows[person_id] = idx_row  # Update row labels\n",
    "    for ftr in ftrs:  # Update data objects\n",
    "        idx_col = feature_columns[ftr]\n",
    "        data_row.append(idx_row)\n",
    "        data_col.append(idx_col)\n",
    "        data.append(1)\n",
    "        \n",
    "# Create a sparse dataframe\n",
    "feature_matrix = scipy.sparse.csr_matrix(\n",
    "    (data, (data_row, data_col)), shape=(len(feature_rows), len(feature_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating labels\n",
    "\n",
    "We need labels for training and testing, but this step is much simpler -- we just need to be certain that our label ordering matches the row ordering in our feature dataframe. We're already using Numpy, Scipy, and Scikit-Learn, so we'll use a Pandas series so that no scientific libraries feel left out.\n",
    "\n",
    "Ratings is simply a data object that was created when accessing data during the preliminary steps that I glossed over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = pd.Series([ratings[person_id] for person_id, _\n",
    "                    in sorted(feature_rows.items(), key=lambda x: x[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generating a classification model\n",
    "\n",
    "After all of this data engineering, how well does a naive bayes model predict our labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import sklearn.naive_bayes\n",
    "\n",
    "\n",
    "def calculate_model_performance(feature_matrix, labels):\n",
    "    # Divide the features and labels into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = sklearn.cross_validation.train_test_split(\n",
    "        feature_matrix, labels)\n",
    "    # Fit a model\n",
    "    model = sklearn.naive_bayes.BernoulliNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    # Predict test set labels\n",
    "    predictions = model.predict(x_test)\n",
    "    r, p, f, _ = sklearn.metrics.precision_recall_fscore_support(\n",
    "        y_test, predictions, pos_label='APPROVED', average='binary')\n",
    "    return r, p, f\n",
    "\n",
    "\n",
    "r, p, f = calculate_model_performance(feature_matrix, labels)\n",
    "print('Recall:     {:.2f} \\nPrecision:  {:.2f} \\nF-Score:    {:.2f}'.format(r, p, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ick.\n",
    "\n",
    "That's not what we were hoping for. Maybe we just got unlucky? Let's cross-validate the model and visualize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "NUM_ITER = 100\n",
    "\n",
    "\n",
    "def cross_validate_model_performance(feature_matrix, labels):\n",
    "    # Repeat training and testing across many data partitions\n",
    "    metrics = [calculate_model_performance(feature_matrix, labels)\n",
    "               for n in range(NUM_ITER)]\n",
    "    # Format data for plotting\n",
    "    performance = [[metrics[idx_iter][idx_metric] for idx_iter in range(len(metrics))]\n",
    "                   for idx_metric in range(3)]\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.boxplot(performance)\n",
    "    plt.title('Model performance')\n",
    "    plt.xticks(range(1, len(performance)+1), ['Recall', 'Precision', 'F-Score'])\n",
    "    plt.ylabel('Percent')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.show()\n",
    "    return performance\n",
    "    \n",
    "\n",
    "performance = cross_validate_model_performance(feature_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Well, it looks like this model isn't fantastic. What if we do feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SUPPORT_THRESHOLD = 0.25\n",
    "\n",
    "\n",
    "def get_supported_features(features, outcomes, prop_train, k_best):\n",
    "    # Create a selector object to calculate feature support\n",
    "    selector = sklearn.feature_selection.SelectKBest(k=k_best)\n",
    "    # Get feature support for different sets of training data\n",
    "    supports = [_get_support(features, outcomes, selector, prop_train)\n",
    "                for _ in range(NUM_ITER)]\n",
    "    # Get features that were supported across at least X% of training sets\n",
    "    return _get_support_summary(supports)\n",
    "\n",
    "\n",
    "def _get_support(features, outcomes, selector, prop_train):\n",
    "    num = features.shape[0]\n",
    "    idx_train = random.sample(range(num), int(num * prop_train))\n",
    "    selector.fit(features[idx_train, :], outcomes[idx_train])\n",
    "    return selector.get_support()\n",
    "\n",
    "\n",
    "def _get_support_summary(supports):\n",
    "    total_support = supports[0].astype(int)\n",
    "    for support in supports[1:]:\n",
    "        total_support += support.astype(int)\n",
    "    return total_support >= SUPPORT_THRESHOLD * len(supports)\n",
    "\n",
    "\n",
    "prop_train = 0.8\n",
    "k = int(0.1 * feature_matrix.shape[1])\n",
    "\n",
    "support = get_supported_features(feature_matrix, outcomes, prop_train, k)\n",
    "performance_selected = cross_validate_model_performance(feature_matrix[support], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(performance, positions=np.arange(1, 8, 2.5))\n",
    "plt.boxplot(performance_selected, positions=np.arange(2, 9, 2.5))\n",
    "plt.title('Model performance')\n",
    "plt.xticks(np.concatenate([np.arange(1, 8, 2.5),np.arange(2, 9, 2.5)]),\n",
    "           ['Recall', 'Precision', 'F-Score'] + ['Pruned'] * 3)\n",
    "plt.xlim([0, 8])\n",
    "plt.ylabel('Percent')\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Hmm, it looks like features selection doesn't make much of a difference and our model still isn't meeting expectations. We have several options for moving forward:  e.g., explore alternative feature selection methods, clean or normalize features, reconsider the labels, choose a different model type.\n",
    "\n",
    "Taking a step back, the issue here is that our labels are determined by several variables, and our feature dataset is not capturing enough of that complexity. We can greatly improve model performance by adding just three additional features, achieving recall and precision scores above 0.9 and F-scores above 0.8, and we can get modest improvements by adding another small set of carefully selected features.\n",
    "\n",
    "The final model is a random forest ensemble model with a handful of quantitative features parsed from the raw data, probability estimates from the above naive bayes model that predicts labels using common keywords, and probability estimates from three naive bayes models that predict labels using three different subsets of technical keywords."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
